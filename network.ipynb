{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fa7dd76",
   "metadata": {},
   "source": [
    "# Network Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec9462f6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.distributions.categorical import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be20e43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork(nn.Module):\n",
    "    \"\"\"LSTM RNN for generating words for wordle solver\"\"\"\n",
    "    def __init__(self, input_size, hidden_size, projection=1, num_layers=5, dropout_probability=0):\n",
    "        super().__init__()\n",
    "        self.network = nn.LSTM(input_size, hidden_size, num_layers, dropout=dropout_probability, proj_size=projection, batch_first=True)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    def forward(self, x):\n",
    "        output, (hidden, _) = self.network(x)\n",
    "        probab = hidden + 1 / 2  # takes output of softmax [-1, 1] and squish to [0, 1]\n",
    "        return probab \n",
    "\n",
    "class CriticNet(nn.Module):\n",
    "    \"\"\"Network representing the critic\"\"\"\n",
    "    def __init__(self, in_shape):\n",
    "        super().__init__()\n",
    "        self.v_network = nn.Sequential(\n",
    "            nn.Linear(in_shape, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    ")\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.v_network(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4969c7d",
   "metadata": {},
   "source": [
    "# Toy Problem for RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "991dadc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.7692, 0.5746, 0.4195, 0.4146, 0.4210, 0.3816, 0.3989, 0.5811,\n",
       "          0.4722, 0.3840, 0.3775, 0.6665, 0.5746, 0.5107, 0.7378, 0.5991,\n",
       "          0.4542, 0.3644, 0.3953, 0.5325, 0.3956, 0.3932, 0.4392, 0.5045,\n",
       "          0.4452, 0.4789]],\n",
       "\n",
       "        [[0.5529, 0.5420, 0.4617, 0.5714, 0.5202, 0.4860, 0.5416, 0.4653,\n",
       "          0.5087, 0.5563, 0.4348, 0.4982, 0.5486, 0.4420, 0.5169, 0.4578,\n",
       "          0.4986, 0.5326, 0.5001, 0.4297, 0.4435, 0.5217, 0.4514, 0.5539,\n",
       "          0.5285, 0.4768]],\n",
       "\n",
       "        [[0.4792, 0.4948, 0.5058, 0.4502, 0.5432, 0.4950, 0.4058, 0.5117,\n",
       "          0.5769, 0.5007, 0.4828, 0.5071, 0.4394, 0.4662, 0.5303, 0.4848,\n",
       "          0.5588, 0.5205, 0.5483, 0.4121, 0.4838, 0.4860, 0.5342, 0.4816,\n",
       "          0.4495, 0.4791]],\n",
       "\n",
       "        [[0.4992, 0.4535, 0.4806, 0.5109, 0.4697, 0.4390, 0.4651, 0.5697,\n",
       "          0.4231, 0.4836, 0.4660, 0.4794, 0.5477, 0.5302, 0.5063, 0.4968,\n",
       "          0.4790, 0.4739, 0.5020, 0.4796, 0.4986, 0.4689, 0.4918, 0.4997,\n",
       "          0.4896, 0.5093]],\n",
       "\n",
       "        [[0.5153, 0.4546, 0.5598, 0.4903, 0.4909, 0.4874, 0.5210, 0.5346,\n",
       "          0.5078, 0.4713, 0.4459, 0.4914, 0.5097, 0.4711, 0.5036, 0.4824,\n",
       "          0.4977, 0.5138, 0.5272, 0.5351, 0.5547, 0.4847, 0.5750, 0.5711,\n",
       "          0.5030, 0.4823]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = ActorNetwork(1, 32, 26, 5)\n",
    "input = torch.tensor([[\n",
    "    [1],\n",
    "    [2],\n",
    "    [3],\n",
    "    [4],\n",
    "    [5]\n",
    "]]).float()\n",
    "print(input.shape)\n",
    "net(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5121a436",
   "metadata": {},
   "source": [
    "# Figuring out the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e07a270c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_wordle\n",
    "wordle = gym.make(\"Wordle-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66da4d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###################################################\n",
      "c r a t e \n",
      "\n",
      "a b c d e f g h i j k l m n o p q r s t u v w x y z \n",
      "###################################################\n",
      "\n",
      "({'board': array([[ 0,  0,  0,  2,  0],\n",
      "       [-1, -1, -1, -1, -1],\n",
      "       [-1, -1, -1, -1, -1],\n",
      "       [-1, -1, -1, -1, -1],\n",
      "       [-1, -1, -1, -1, -1],\n",
      "       [-1, -1, -1, -1, -1]]), 'alphabet': array([ 0, -1,  0, -1,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        0, -1,  2, -1, -1, -1, -1, -1, -1])}, 0.0, False, {})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(24, 14, 20, 19, 7)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordle.reset()\n",
    "obs = wordle.step([2, 17, 0, 19, 4])\n",
    "wordle.render()\n",
    "print(obs)\n",
    "wordle.hidden_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a8c7e62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'board': array([[ 0,  0,  0,  2,  0],\n",
       "         [ 0,  0,  0,  0,  0],\n",
       "         [-1, -1, -1, -1, -1],\n",
       "         [-1, -1, -1, -1, -1],\n",
       "         [-1, -1, -1, -1, -1],\n",
       "         [-1, -1, -1, -1, -1]]),\n",
       "  'alphabet': array([ 0,  0,  0, -1,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "          0, -1,  2, -1, -1, -1, -1, -1, -1])},\n",
       " 0.0,\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordle.step([17, 4, 1, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b067beb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 5])\n",
      "tensor([[[0.5784, 0.5548, 0.4521, 0.5546, 0.4620, 0.3900, 0.4581, 0.4895,\n",
      "          0.3628, 0.6697, 0.5521, 0.5222, 0.5605, 0.5359, 0.5742, 0.4070,\n",
      "          0.5257, 0.4200, 0.4461, 0.5238, 0.3695, 0.4998, 0.4732, 0.4871,\n",
      "          0.5293, 0.4226]],\n",
      "\n",
      "        [[0.5291, 0.4355, 0.4914, 0.5404, 0.5746, 0.4720, 0.4706, 0.4990,\n",
      "          0.5314, 0.5156, 0.5034, 0.4374, 0.5129, 0.4606, 0.5518, 0.4645,\n",
      "          0.4489, 0.5344, 0.5514, 0.4465, 0.4661, 0.4793, 0.4780, 0.5296,\n",
      "          0.4005, 0.5002]],\n",
      "\n",
      "        [[0.4930, 0.5260, 0.4909, 0.3779, 0.4686, 0.4809, 0.5326, 0.5496,\n",
      "          0.5125, 0.5935, 0.4286, 0.5348, 0.4980, 0.4997, 0.4767, 0.5332,\n",
      "          0.5729, 0.4449, 0.5696, 0.4686, 0.5495, 0.5180, 0.5247, 0.4759,\n",
      "          0.4945, 0.4484]],\n",
      "\n",
      "        [[0.5176, 0.4395, 0.4981, 0.4888, 0.5461, 0.5065, 0.5027, 0.4784,\n",
      "          0.4654, 0.5807, 0.5061, 0.5163, 0.5095, 0.4966, 0.5762, 0.4662,\n",
      "          0.4296, 0.5343, 0.4741, 0.5453, 0.5063, 0.4983, 0.4748, 0.4978,\n",
      "          0.5203, 0.5480]],\n",
      "\n",
      "        [[0.4975, 0.4860, 0.4838, 0.5524, 0.5037, 0.5136, 0.4951, 0.4459,\n",
      "          0.5011, 0.4937, 0.5543, 0.4990, 0.4920, 0.4733, 0.5313, 0.4821,\n",
      "          0.4587, 0.4880, 0.5145, 0.5119, 0.5223, 0.5003, 0.4681, 0.5033,\n",
      "          0.5197, 0.5529]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "rnn = ActorNetwork(5, 32, 26, 5)\n",
    "o = np.array([obs[0]['board']])\n",
    "o = torch.from_numpy(o).to(torch.float32)\n",
    "print(o.shape)\n",
    "print(rnn(o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b21e1e9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7],\n",
       "        [ 8],\n",
       "        [12],\n",
       "        [ 4],\n",
       "        [20]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob = Categorical(rnn(o))\n",
    "prob.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c752e545",
   "metadata": {},
   "source": [
    "# Competing A2C solvers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0234fba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(replay, q_val):\n",
    "    vals = torch.vstack(replay.vals)\n",
    "    \n",
    "    rewards_tensor = torch.tensor(np.asarray(replay.rewards, dtype=np.float32))\n",
    "    is_terminal_tensor = torch.tensor(np.asarray(replay.dones, dtype=np.int32))\n",
    "        \n",
    "    q_vals_tensor = rewards_tensor + discount_factor * q_val * (1 - is_terminal_tensor)\n",
    "        \n",
    "    advantage = q_vals_tensor - vals\n",
    "    \n",
    "    critic_loss = (advantage ** 2).mean()\n",
    "    # critic_loss.requires_grad = True\n",
    "    adam_critic.zero_grad()\n",
    "    critic_loss.backward(retain_graph=True)\n",
    "    adam_critic.step()\n",
    "    \n",
    "    \n",
    "    log_probabs_tensor = torch.vstack(replay.log_probs)\n",
    "    actor_loss = (-log_probabs_tensor*advantage.detach()).mean()\n",
    "    # actor_loss.requires_grad = True\n",
    "    adam_actor.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    adam_actor.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "61b5c6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Advantage_ActorCritic():\n",
    "    def __init__(self, world: gym.Env, policy_net: ActorNetwork, critic_net: CriticNet,\n",
    "                 encoder, policy_alpha, critic_alpha, gamma, max_reward):\n",
    "        # environment info\n",
    "        self.world = world\n",
    "        self.encoder = encoder\n",
    "        self.max_reward = max_reward\n",
    "\n",
    "        # actor and critic\n",
    "        self.actor = policy_net\n",
    "        self.critic = critic_net\n",
    "        self.error_buffer = list()\n",
    "        self.policy_optimizer = torch.optim.Adam(policy_net.network.parameters(), lr=policy_alpha)\n",
    "        self.v_optimizer = torch.optim.Adam(critic_net.v_network.parameters(), lr=critic_alpha)\n",
    "\n",
    "        # training info\n",
    "        self.gamma = gamma\n",
    "        self.episodes = 0\n",
    "\n",
    "    def train(self, iterations):\n",
    "        converged = False\n",
    "        rewards = list()\n",
    "        recents = torch.zeros(10)\n",
    "        i = 0\n",
    "        while not converged:\n",
    "            r = self.episode()\n",
    "            rewards.append(r)\n",
    "\n",
    "            if i < 10:\n",
    "                recents[i] = r\n",
    "                i += 1\n",
    "\n",
    "            else:\n",
    "                recents.roll(-1, 0)\n",
    "                recents[9] = r\n",
    "            \n",
    "            # convergence check\n",
    "            if len(rewards) > 10 and recents.mean() >= self.max_reward:\n",
    "                converged = True\n",
    "            converged = True if iterations == self.episodes else False\n",
    "        \n",
    "        plt.plot(rewards)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def episode(self, training=True):\n",
    "        done = False\n",
    "        state = self.world.reset().copy()\n",
    "        episode_reward = 0\n",
    "        self.episodes += 1\n",
    "        while not done:\n",
    "            # take on policy action\n",
    "            encoded = self.encoder(state)\n",
    "            net_out = self.actor(encoded)\n",
    "\n",
    "            dist = Categorical(net_out)\n",
    "            action = dist.sample()\n",
    "            try:\n",
    "                state_prime, reward, done, _ = self.world.step(action)\n",
    "            except AssertionError:\n",
    "                reward = -.1\n",
    "                state_prime = state.copy()\n",
    "\n",
    "            # fill buffer\n",
    "            self.error_buffer.append((state, state_prime, reward, dist.log_prob(action)))\n",
    "\n",
    "            # prepare for next iteration\n",
    "            episode_reward += reward\n",
    "            state = state_prime.copy()\n",
    "            if training:\n",
    "                self.__net_update()\n",
    "\n",
    "        return episode_reward\n",
    "\n",
    "    def td_error(self, value, value_prime, reward):\n",
    "        def loss_fn():\n",
    "            print(value.shape, value_prime.shape) \n",
    "            return reward + self.gamma * value_prime - value\n",
    "        return loss_fn()\n",
    "\n",
    "    def policy_error(self, prob, error):\n",
    "        def loss_fn():\n",
    "            print(prob)\n",
    "            print(error)\n",
    "            return -torch.log(prob) * error\n",
    "        return loss_fn()\n",
    "\n",
    "    def __net_update(self):\n",
    "        # calculate error for policy and critic\n",
    "        self.critic.v_network.zero_grad()\n",
    "        self.actor.network.zero_grad()\n",
    "\n",
    "        state, state_prime, reward, prob = self.error_buffer.pop(0)\n",
    "        print(self.encoder(state))\n",
    "        value = self.critic(self.encoder(state))\n",
    "        value_prime = self.critic(self.encoder(state_prime))\n",
    "\n",
    "        td_error = self.td_error(value, value_prime, reward)\n",
    "        policy_error = self.policy_error(prob, td_error) \n",
    "\n",
    "        # backprop\n",
    "        if self.episodes > 100:\n",
    "            td_error.backward()\n",
    "        else:\n",
    "            policy_error.backward()\n",
    "            self.policy_optimizer.step()\n",
    "        self.v_optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1ddb08",
   "metadata": {},
   "source": [
    "# Solving the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d00b2396",
   "metadata": {},
   "outputs": [],
   "source": [
    "critic = CriticNet(30)\n",
    "actor = ActorNetwork(30, 32, 26)\n",
    "agent = Advantage_ActorCritic(wordle, actor, critic, \n",
    "lambda x: torch.tensor([x['board']]).to(torch.float32).reshape(1, 1,30), 1e-3, 1e-3, 0.9, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1aacca98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "          -1., -1.]]])\n",
      "torch.Size([1, 1, 1]) torch.Size([1, 1, 1])\n",
      "tensor([[-3.1386],\n",
      "        [-3.2099],\n",
      "        [-3.1924],\n",
      "        [-3.2840],\n",
      "        [-3.2970]], grad_fn=<SqueezeBackward1>)\n",
      "tensor([[[-0.0904]]], grad_fn=<SubBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/jbreindl/Documents/university/spring_2022/546/wordle-rl-bot/network.ipynb Cell 17'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jbreindl/Documents/university/spring_2022/546/wordle-rl-bot/network.ipynb#ch0000012?line=0'>1</a>\u001b[0m agent\u001b[39m.\u001b[39;49mepisode()\n",
      "\u001b[1;32m/Users/jbreindl/Documents/university/spring_2022/546/wordle-rl-bot/network.ipynb Cell 14'\u001b[0m in \u001b[0;36mAdvantage_ActorCritic.episode\u001b[0;34m(self, training)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jbreindl/Documents/university/spring_2022/546/wordle-rl-bot/network.ipynb#ch0000009?line=68'>69</a>\u001b[0m     state \u001b[39m=\u001b[39m state_prime\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jbreindl/Documents/university/spring_2022/546/wordle-rl-bot/network.ipynb#ch0000009?line=69'>70</a>\u001b[0m     \u001b[39mif\u001b[39;00m training:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jbreindl/Documents/university/spring_2022/546/wordle-rl-bot/network.ipynb#ch0000009?line=70'>71</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__net_update()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jbreindl/Documents/university/spring_2022/546/wordle-rl-bot/network.ipynb#ch0000009?line=72'>73</a>\u001b[0m \u001b[39mreturn\u001b[39;00m episode_reward\n",
      "\u001b[1;32m/Users/jbreindl/Documents/university/spring_2022/546/wordle-rl-bot/network.ipynb Cell 14'\u001b[0m in \u001b[0;36mAdvantage_ActorCritic.__net_update\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jbreindl/Documents/university/spring_2022/546/wordle-rl-bot/network.ipynb#ch0000009?line=102'>103</a>\u001b[0m     td_error\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jbreindl/Documents/university/spring_2022/546/wordle-rl-bot/network.ipynb#ch0000009?line=103'>104</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/jbreindl/Documents/university/spring_2022/546/wordle-rl-bot/network.ipynb#ch0000009?line=104'>105</a>\u001b[0m     policy_error\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jbreindl/Documents/university/spring_2022/546/wordle-rl-bot/network.ipynb#ch0000009?line=105'>106</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy_optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jbreindl/Documents/university/spring_2022/546/wordle-rl-bot/network.ipynb#ch0000009?line=106'>107</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv_optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/tensor.py:245\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/tensor.py?line=235'>236</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/tensor.py?line=236'>237</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/tensor.py?line=237'>238</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/tensor.py?line=238'>239</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/tensor.py?line=242'>243</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/tensor.py?line=243'>244</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> <a href='file:///opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/tensor.py?line=244'>245</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/autograd/__init__.py:141\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/autograd/__init__.py?line=137'>138</a>\u001b[0m inputs \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(inputs) \u001b[39mif\u001b[39;00m inputs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mtuple\u001b[39m()\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/autograd/__init__.py?line=139'>140</a>\u001b[0m grad_tensors_ \u001b[39m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[39mlen\u001b[39m(tensors))\n\u001b[0;32m--> <a href='file:///opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/autograd/__init__.py?line=140'>141</a>\u001b[0m grad_tensors_ \u001b[39m=\u001b[39m _make_grads(tensors, grad_tensors_)\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/autograd/__init__.py?line=141'>142</a>\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/autograd/__init__.py?line=142'>143</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/autograd/__init__.py:50\u001b[0m, in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads)\u001b[0m\n\u001b[1;32m     <a href='file:///opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/autograd/__init__.py?line=47'>48</a>\u001b[0m \u001b[39mif\u001b[39;00m out\u001b[39m.\u001b[39mrequires_grad:\n\u001b[1;32m     <a href='file:///opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/autograd/__init__.py?line=48'>49</a>\u001b[0m     \u001b[39mif\u001b[39;00m out\u001b[39m.\u001b[39mnumel() \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> <a href='file:///opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/autograd/__init__.py?line=49'>50</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='file:///opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/autograd/__init__.py?line=50'>51</a>\u001b[0m     new_grads\u001b[39m.\u001b[39mappend(torch\u001b[39m.\u001b[39mones_like(out, memory_format\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mpreserve_format))\n\u001b[1;32m     <a href='file:///opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/autograd/__init__.py?line=51'>52</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "agent.episode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03433a61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c6e4e9f98eb68ad3b7c296f83d20e6de614cb42e90992a65aa266555a3137d0d"
  },
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "py:light,ipynb",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
