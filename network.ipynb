{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fa7dd76",
   "metadata": {},
   "source": [
    "# Network Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "569ee78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch ./Gym-Wordle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec9462f6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be20e43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class ActorNetwork(nn.Module):\n",
    "    \"\"\"LSTM RNN for generating words for wordle solver\"\"\"\n",
    "    def __init__(self, input_size, output_size=2):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_size, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, output_size),\n",
    "        )\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    def forward(self, x):\n",
    "        y = self.network(x)\n",
    "        return y\n",
    "\n",
    "class CriticNet(nn.Module):\n",
    "    \"\"\"Network representing the critic\"\"\"\n",
    "    def __init__(self, in_shape):\n",
    "        super().__init__()\n",
    "        self.v_network = nn.Sequential(\n",
    "            nn.Linear(in_shape, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.v_network(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5121a436",
   "metadata": {},
   "source": [
    "# Figuring out the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e07a270c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_wordle.utils\n",
    "import numpy as np\n",
    "wordle = gym.make('Wordle-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c752e545",
   "metadata": {},
   "source": [
    "# Competing A2C solvers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0234fba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(replay, q_val):\n",
    "    vals = torch.vstack(replay.vals)\n",
    "    \n",
    "    rewards_tensor = torch.tensor(np.asarray(replay.rewards, dtype=np.float32))\n",
    "    is_terminal_tensor = torch.tensor(np.asarray(replay.dones, dtype=np.int32))\n",
    "        \n",
    "    q_vals_tensor = rewards_tensor + discount_factor * q_val * (1 - is_terminal_tensor)\n",
    "        \n",
    "    advantage = q_vals_tensor - vals\n",
    "    \n",
    "    critic_loss = (advantage ** 2).mean()\n",
    "    # critic_loss.requires_grad = True\n",
    "    adam_critic.zero_grad()\n",
    "    critic_loss.backward(retain_graph=True)\n",
    "    adam_critic.step()\n",
    "    \n",
    "    \n",
    "    log_probabs_tensor = torch.vstack(replay.log_probs)\n",
    "    actor_loss = (-log_probabs_tensor*advantage.detach()).mean()\n",
    "    # actor_loss.requires_grad = True\n",
    "    adam_actor.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    adam_actor.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61b5c6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Advantage_ActorCritic():\n",
    "    def __init__(self, world: gym.Env, policy_net: ActorNetwork, critic,\n",
    "                 encoder, policy_alpha,  gamma, max_reward):\n",
    "        # environment info\n",
    "        self.world = world\n",
    "        self.encoder = encoder\n",
    "        self.max_reward = max_reward\n",
    "\n",
    "        # actor and critic\n",
    "        self.actor = policy_net\n",
    "        self.critic = critic\n",
    "        self.error_buffer = list()\n",
    "        self.policy_optimizer = torch.optim.Adam(policy_net.network.parameters(), lr=policy_alpha)\n",
    "\n",
    "        # training info\n",
    "        self.gamma = gamma\n",
    "        self.episodes = 0\n",
    "        self.step = -1\n",
    "        self.guesses = np.zeros(world.action_space.n)\n",
    "\n",
    "    def train(self, iterations):\n",
    "        converged = False\n",
    "        rewards = list()\n",
    "        recents = torch.zeros(10)\n",
    "        i = 0\n",
    "        while not converged:\n",
    "            r = self.episode()\n",
    "            rewards.append(r)\n",
    "\n",
    "            if i < 10:\n",
    "                recents[i] = r\n",
    "                i += 1\n",
    "\n",
    "            else:\n",
    "                recents.roll(-1, 0)\n",
    "                recents[9] = r\n",
    "            \n",
    "            # convergence check\n",
    "            if len(rewards) > 10 and recents.mean() >= self.max_reward:\n",
    "                converged = True\n",
    "            converged = True if iterations == self.episodes else False\n",
    "        \n",
    "        plt.plot(rewards)\n",
    "        plt.show()\n",
    "\n",
    "    def episode(self, training=True):\n",
    "        done = False\n",
    "        state = self.world.reset().copy()\n",
    "        episode_reward = 0\n",
    "        self.episodes += 1\n",
    "        self.step = -1\n",
    "        while not done:\n",
    "            self.step += 1\n",
    "            # take on policy action\n",
    "            encoded = self.encoder(state)\n",
    "            dist = self.actor(encoded)\n",
    "\n",
    "            action = normal(dist[0], dist[1])\n",
    "            action = int(action.clip(0, self.world.action_space.n-1).round().item())\n",
    "            self.guesses[action] += 1\n",
    "            state_prime, reward, done, _ = self.world.step(action)\n",
    "\n",
    "            # fill buffer\n",
    "            self.error_buffer.append((state, state_prime, reward, dist))\n",
    "\n",
    "            # prepare for next iteration\n",
    "            episode_reward += reward\n",
    "            state = state_prime.copy()\n",
    "            if training:\n",
    "                self.__net_update()\n",
    "\n",
    "        return episode_reward\n",
    "\n",
    "    # Actor error\n",
    "    def policy_error(self, prob, error):\n",
    "        log_probabs_tensor = prob.reshape(-1)\n",
    "        return (-log_probabs_tensor * error).mean()\n",
    "\n",
    "    def __net_update(self):\n",
    "        # calculate error for policy and critic\n",
    "        self.actor.network.zero_grad()\n",
    "\n",
    "        state, state_prime, reward, prob = self.error_buffer.pop(0)\n",
    "        loss = self.critic(state_prime[self.step])\n",
    "        policy_error = self.policy_error(prob, loss) \n",
    "\n",
    "        # backprop\n",
    "        policy_error.backward()\n",
    "        self.policy_optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44cf189",
   "metadata": {},
   "source": [
    "# Use entropy minimization to develop a critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eaa9483f",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropies = list()\n",
    "def entropy_critic(guess: np.ndarray):\n",
    "\tremaining_space = gym_wordle.utils.get_words(\"guess\")\n",
    "\t# get guess and the values\n",
    "\tsplit = np.split(guess, 2)\n",
    "\tg = split[0]\n",
    "\tvals = split[1]\n",
    "\tprint(g, vals)\n",
    "\n",
    "\t# get complete matches (i.e. matches based on position)\n",
    "\tmatches = np.argwhere(vals==1).flatten()\n",
    "\tprint(matches)\n",
    "\tfor i in matches:\n",
    "\t\t#print(np.nonzero(remaining_space[:, i] == g[i]))\n",
    "\t\tremaining_space = remaining_space[np.nonzero(remaining_space[:, i] == g[i])] \n",
    "\tprint(remaining_space.shape)\n",
    "\t# find remaining words that this letter\n",
    "\texists = np.argwhere(vals==2).flatten()\n",
    "\tprint(exists)\n",
    "\tfor i in exists:\n",
    "\t\tprint(\"letter\", g[i])\n",
    "\t\tprint(np.any(remaining_space, 1, where=g[i]).shape , \" exist\")\n",
    "\t\tremaining_space = remaining_space[np.any(remaining_space, 1, where=g[i])]\n",
    "\tprint(remaining_space.shape)\n",
    "\n",
    "\t# we know what letters we DON'T have\n",
    "\tdne = np.argwhere(vals==3).flatten()\n",
    "\tprint(dne)\n",
    "\tfor i in dne:\n",
    "\t\tprint(np.any(remaining_space, 1, where=g[i]).shape , \"dne\")  # this is returning entire dictionary\n",
    "\t\tremaining_space = remaining_space[np.logical_not(np.any(remaining_space, 1, where=g[i]))]\n",
    "\n",
    "\t# do calc\n",
    "\tp = float(remaining_space.shape[0]) / float(wordle.action_space.n)\n",
    "\tprint(p)\n",
    "\tentropy = -p * np.log(p)\n",
    "\tentropies.append(entropy)\n",
    "\treturn entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28c225b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AA\u001b[43mH\u001b[49m\u001b[43mE\u001b[49mD\n",
      "AA\u001b[43mH\u001b[49m\u001b[43mE\u001b[49mD\n",
      "AA\u001b[43mH\u001b[49m\u001b[43mE\u001b[49mD\n",
      "     \n",
      "     \n",
      "     \n",
      "[1 1 8 5 4] [3 3 2 2 3]\n",
      "[]\n",
      "(12972, 5)\n",
      "[2 3]\n",
      "letter 8\n",
      "(12972,)  exist\n",
      "letter 5\n",
      "(12972,)  exist\n",
      "(12972, 5)\n",
      "[0 1 4]\n",
      "(12972,) dne\n",
      "(0,) dne\n",
      "(0,) dne\n",
      "0.0\n",
      "AA\u001b[43mH\u001b[49m\u001b[43mE\u001b[49mD\n",
      "AA\u001b[43mH\u001b[49m\u001b[43mE\u001b[49mD\n",
      "AA\u001b[43mH\u001b[49m\u001b[43mE\u001b[49mD\n",
      "     \n",
      "     \n",
      "     \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pz/mmdg2qc54x9gxppy369prgy80000gn/T/ipykernel_97099/4151420395.py:37: RuntimeWarning: divide by zero encountered in log\n",
      "  entropy = -p * np.log(p)\n",
      "/var/folders/pz/mmdg2qc54x9gxppy369prgy80000gn/T/ipykernel_97099/4151420395.py:37: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  entropy = -p * np.log(p)\n"
     ]
    }
   ],
   "source": [
    "#wordle.reset()\n",
    "#obs = wordle.step(0)\n",
    "wordle.render()\n",
    "entropy_critic(obs[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1ddb08",
   "metadata": {},
   "source": [
    "# Solving the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d00b2396",
   "metadata": {},
   "outputs": [],
   "source": [
    "critic = entropy_critic\n",
    "actor = ActorNetwork(60, 2)\n",
    "agent = Advantage_ActorCritic(wordle, actor, critic, \n",
    "lambda x: torch.tensor(x.flatten()).to(torch.float32), 1e-3, 0.9, -4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3cd11b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 8 5 4] [3 3 2 2 3]\n",
      "[]\n",
      "(12972, 5)\n",
      "[2 3]\n",
      "letter 8\n",
      "(12972,)  exist\n",
      "letter 5\n",
      "(12972,)  exist\n",
      "(12972, 5)\n",
      "[0 1 4]\n",
      "(12972,) dne\n",
      "(0,) dne\n",
      "(0,) dne\n",
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pz/mmdg2qc54x9gxppy369prgy80000gn/T/ipykernel_97099/4151420395.py:37: RuntimeWarning: divide by zero encountered in log\n",
      "  entropy = -p * np.log(p)\n",
      "/var/folders/pz/mmdg2qc54x9gxppy369prgy80000gn/T/ipykernel_97099/4151420395.py:37: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  entropy = -p * np.log(p)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot convert float NaN to integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/jbreindl/Documents/university/spring_2022/546/wordle-rl-bot/network.ipynb Cell 15'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jbreindl/Documents/university/spring_2022/546/wordle-rl-bot/network.ipynb#ch0000015?line=0'>1</a>\u001b[0m agent\u001b[39m.\u001b[39;49mepisode()\n",
      "\u001b[1;32m/Users/jbreindl/Documents/university/spring_2022/546/wordle-rl-bot/network.ipynb Cell 9'\u001b[0m in \u001b[0;36mAdvantage_ActorCritic.episode\u001b[0;34m(self, training)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jbreindl/Documents/university/spring_2022/546/wordle-rl-bot/network.ipynb#ch0000010?line=55'>56</a>\u001b[0m dist \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactor(encoded)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jbreindl/Documents/university/spring_2022/546/wordle-rl-bot/network.ipynb#ch0000010?line=57'>58</a>\u001b[0m action \u001b[39m=\u001b[39m normal(dist[\u001b[39m0\u001b[39m], dist[\u001b[39m1\u001b[39m])\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jbreindl/Documents/university/spring_2022/546/wordle-rl-bot/network.ipynb#ch0000010?line=58'>59</a>\u001b[0m action \u001b[39m=\u001b[39m \u001b[39mint\u001b[39;49m(action\u001b[39m.\u001b[39;49mclip(\u001b[39m0\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mworld\u001b[39m.\u001b[39;49maction_space\u001b[39m.\u001b[39;49mn\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39;49mround()\u001b[39m.\u001b[39;49mitem())\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jbreindl/Documents/university/spring_2022/546/wordle-rl-bot/network.ipynb#ch0000010?line=59'>60</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mguesses[action] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jbreindl/Documents/university/spring_2022/546/wordle-rl-bot/network.ipynb#ch0000010?line=60'>61</a>\u001b[0m state_prime, reward, done, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworld\u001b[39m.\u001b[39mstep(action)\n",
      "\u001b[0;31mValueError\u001b[0m: cannot convert float NaN to integer"
     ]
    }
   ],
   "source": [
    "agent.episode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53a8914",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.train(2000)\n",
    "plt.plot(entropies)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ce28ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6a0a884f1235414abea8799649159e2407a8ec7e78a1569e1fef1695eed75495"
  },
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "py:light,ipynb",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('tf')",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
