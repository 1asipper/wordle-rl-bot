{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fa7dd76",
   "metadata": {},
   "source": [
    "# Network Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569ee78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch ./Gym-Wordle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec9462f6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.distributions.categorical import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be20e43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork(nn.Module):\n",
    "    \"\"\"LSTM RNN for generating words for wordle solver\"\"\"\n",
    "    def __init__(self, input_size, hidden_size, projection=1, num_layers=5, dropout_probability=0):\n",
    "        super().__init__()\n",
    "        self.network = nn.LSTM(input_size, hidden_size, num_layers, dropout=dropout_probability, proj_size=projection, batch_first=True)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    def forward(self, x):\n",
    "        output, (hidden, _) = self.network(x)\n",
    "        probab = hidden + 1 / 2  # takes output of softmax [-1, 1] and squish to [0, 1]\n",
    "        return probab \n",
    "\n",
    "class CriticNet(nn.Module):\n",
    "    \"\"\"Network representing the critic\"\"\"\n",
    "    def __init__(self, in_shape):\n",
    "        super().__init__()\n",
    "        self.v_network = nn.Sequential(\n",
    "            nn.Linear(in_shape, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    ")\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.v_network(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4969c7d",
   "metadata": {},
   "source": [
    "# Toy Problem for RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "991dadc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2923, 0.1851, 0.4920, 0.3597, 0.8412, 0.6158, 0.1874, 0.5343,\n",
       "          0.4733, 0.5026, 0.7141, 0.4920, 0.6838, 0.5111, 0.2538, 0.2360,\n",
       "          0.3221, 0.4001, 0.5565, 0.4178, 0.5576, 0.4621, 0.3749, 0.3180,\n",
       "          0.4408, 0.3913]],\n",
       "\n",
       "        [[0.5561, 0.5760, 0.4234, 0.4417, 0.5697, 0.5104, 0.5349, 0.5507,\n",
       "          0.5039, 0.6347, 0.4169, 0.4416, 0.5717, 0.5164, 0.5957, 0.4740,\n",
       "          0.5279, 0.4453, 0.5402, 0.5396, 0.4736, 0.4346, 0.4352, 0.5314,\n",
       "          0.4301, 0.4873]],\n",
       "\n",
       "        [[0.5211, 0.5036, 0.4526, 0.5146, 0.5261, 0.5181, 0.5902, 0.4431,\n",
       "          0.5116, 0.4362, 0.4546, 0.5290, 0.4591, 0.4928, 0.3859, 0.5028,\n",
       "          0.6048, 0.5058, 0.5122, 0.5380, 0.5071, 0.4984, 0.6031, 0.4086,\n",
       "          0.5445, 0.5672]],\n",
       "\n",
       "        [[0.5102, 0.5152, 0.5857, 0.4535, 0.5058, 0.4311, 0.5242, 0.5814,\n",
       "          0.5547, 0.5103, 0.5472, 0.5071, 0.4993, 0.4273, 0.5168, 0.5129,\n",
       "          0.4913, 0.5279, 0.4239, 0.5279, 0.5121, 0.4617, 0.5231, 0.4680,\n",
       "          0.5369, 0.5313]],\n",
       "\n",
       "        [[0.5063, 0.5252, 0.4854, 0.4607, 0.4779, 0.5285, 0.4776, 0.5246,\n",
       "          0.4705, 0.4986, 0.5172, 0.4647, 0.5330, 0.5335, 0.4886, 0.5169,\n",
       "          0.4236, 0.4396, 0.5930, 0.5163, 0.4517, 0.5230, 0.4469, 0.5482,\n",
       "          0.4262, 0.5091]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = ActorNetwork(1, 32, 26, 5)\n",
    "input = torch.tensor([[\n",
    "    [1],\n",
    "    [2],\n",
    "    [3],\n",
    "    [4],\n",
    "    [5]\n",
    "]]).float()\n",
    "print(input.shape)\n",
    "net(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5121a436",
   "metadata": {},
   "source": [
    "# Figuring out the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e07a270c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_wordle\n",
    "wordle = gym.make(\"Wordle-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66da4d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AA\u001b[43mR\u001b[49mGH\n",
      "     \n",
      "     \n",
      "     \n",
      "     \n",
      "     \n",
      "[[ 1  1 18  7  8  3  3  2  3  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "obs = wordle.reset()\n",
    "wordle.step(2)\n",
    "wordle.render()\n",
    "print(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c752e545",
   "metadata": {},
   "source": [
    "# Competing A2C solvers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0234fba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(replay, q_val):\n",
    "    vals = torch.vstack(replay.vals)\n",
    "    \n",
    "    rewards_tensor = torch.tensor(np.asarray(replay.rewards, dtype=np.float32))\n",
    "    is_terminal_tensor = torch.tensor(np.asarray(replay.dones, dtype=np.int32))\n",
    "        \n",
    "    q_vals_tensor = rewards_tensor + discount_factor * q_val * (1 - is_terminal_tensor)\n",
    "        \n",
    "    advantage = q_vals_tensor - vals\n",
    "    \n",
    "    critic_loss = (advantage ** 2).mean()\n",
    "    # critic_loss.requires_grad = True\n",
    "    adam_critic.zero_grad()\n",
    "    critic_loss.backward(retain_graph=True)\n",
    "    adam_critic.step()\n",
    "    \n",
    "    \n",
    "    log_probabs_tensor = torch.vstack(replay.log_probs)\n",
    "    actor_loss = (-log_probabs_tensor*advantage.detach()).mean()\n",
    "    # actor_loss.requires_grad = True\n",
    "    adam_actor.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    adam_actor.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b5c6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Advantage_ActorCritic():\n",
    "    def __init__(self, world: gym.Env, policy_net: ActorNetwork, critic_net: CriticNet,\n",
    "                 encoder, policy_alpha, critic_alpha, gamma, max_reward):\n",
    "        # environment info\n",
    "        self.world = world\n",
    "        self.encoder = encoder\n",
    "        self.max_reward = max_reward\n",
    "\n",
    "        # actor and critic\n",
    "        self.actor = policy_net\n",
    "        self.critic = critic_net\n",
    "        self.error_buffer = list()\n",
    "        self.policy_optimizer = torch.optim.Adam(policy_net.network.parameters(), lr=policy_alpha)\n",
    "        self.v_optimizer = torch.optim.Adam(critic_net.v_network.parameters(), lr=critic_alpha)\n",
    "\n",
    "        # training info\n",
    "        self.gamma = gamma\n",
    "        self.episodes = 0\n",
    "\n",
    "    def train(self, iterations):\n",
    "        converged = False\n",
    "        rewards = list()\n",
    "        recents = torch.zeros(10)\n",
    "        i = 0\n",
    "        while not converged:\n",
    "            r = self.episode()\n",
    "            rewards.append(r)\n",
    "\n",
    "            if i < 10:\n",
    "                recents[i] = r\n",
    "                i += 1\n",
    "\n",
    "            else:\n",
    "                recents.roll(-1, 0)\n",
    "                recents[9] = r\n",
    "            \n",
    "            # convergence check\n",
    "            if len(rewards) > 10 and recents.mean() >= self.max_reward:\n",
    "                converged = True\n",
    "            converged = True if iterations == self.episodes else False\n",
    "        \n",
    "        plt.plot(rewards)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def episode(self, training=True):\n",
    "        done = False\n",
    "        state = self.world.reset().copy()\n",
    "        episode_reward = 0\n",
    "        self.episodes += 1\n",
    "        while not done:\n",
    "            # take on policy action\n",
    "            encoded = self.encoder(state)\n",
    "            net_out = self.actor(encoded)\n",
    "\n",
    "            dist = Categorical(net_out)\n",
    "            action = dist.sample()\n",
    "            try:\n",
    "                state_prime, reward, done, _ = self.world.step(action)\n",
    "            except AssertionError:\n",
    "                reward = -.1\n",
    "                state_prime = state.copy()\n",
    "\n",
    "            # fill buffer\n",
    "            self.error_buffer.append((state, state_prime, reward, dist.log_prob(action)))\n",
    "\n",
    "            # prepare for next iteration\n",
    "            episode_reward += reward\n",
    "            state = state_prime.copy()\n",
    "            if training:\n",
    "                self.__net_update()\n",
    "\n",
    "        return episode_reward\n",
    "\n",
    "    def td_error(self, value, value_prime, reward):\n",
    "        def loss_fn():\n",
    "            print(value.shape, value_prime.shape) \n",
    "            return reward + self.gamma * value_prime - value\n",
    "        return loss_fn()\n",
    "\n",
    "    def policy_error(self, prob, error):\n",
    "        def loss_fn():\n",
    "            print(prob)\n",
    "            print(error)\n",
    "            return -torch.log(prob) * error\n",
    "        return loss_fn()\n",
    "\n",
    "    def __net_update(self):\n",
    "        # calculate error for policy and critic\n",
    "        self.critic.v_network.zero_grad()\n",
    "        self.actor.network.zero_grad()\n",
    "\n",
    "        state, state_prime, reward, prob = self.error_buffer.pop(0)\n",
    "        print(self.encoder(state))\n",
    "        value = self.critic(self.encoder(state))\n",
    "        value_prime = self.critic(self.encoder(state_prime))\n",
    "\n",
    "        td_error = self.td_error(value, value_prime, reward)\n",
    "        policy_error = self.policy_error(prob, td_error) \n",
    "\n",
    "        # backprop\n",
    "        if self.episodes > 100:\n",
    "            td_error.backward()\n",
    "        else:\n",
    "            policy_error.backward()\n",
    "            self.policy_optimizer.step()\n",
    "        self.v_optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1ddb08",
   "metadata": {},
   "source": [
    "# Solving the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00b2396",
   "metadata": {},
   "outputs": [],
   "source": [
    "critic = CriticNet(30)\n",
    "actor = ActorNetwork(30, 32, 26)\n",
    "agent = Advantage_ActorCritic(wordle, actor, critic, \n",
    "lambda x: torch.tensor([x['board']]).to(torch.float32).reshape(1, 1,30), 1e-3, 1e-3, 0.9, 1)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c6e4e9f98eb68ad3b7c296f83d20e6de614cb42e90992a65aa266555a3137d0d"
  },
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "py:light,ipynb",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
